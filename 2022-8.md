# 8.22
系统信息：
win+r input : msinfo32, i511400F 6core

bios:传统是MBR

教程:https://www.cnblogs.com/masbay/p/10745170.html

磁盘 属性- 卷 - gpt

// 放弃了，我认为是驱动的问题。 选项 quiet splash 删去之后发现USB串口似乎不能识别，或者是cpu？问题
ubuntu在笔记本上正常工作，问题应该是驱动

安装npm - node.js

安装latex https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/

安装cuda https://blog.csdn.net/m0_45447650/article/details/123704930 
https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/win-64/
https://developer.nvidia.com/rdp/cudnn-download  复制到对应文件

minGW https://osdn.net/projects/mingw/

torch：pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
torch.cuda.is_available()

安装git 
https://blog.csdn.net/weixin_48349367/article/details/120056192

# 8.23
titanic dataset https://www.kaggle.com/competitions/titanic/data?select=train.csv

opencv https://www.raoyunsoft.com/opencv/
cv2 : pip install opencv-python 版本4.6.0
https://blog.csdn.net/xiaoxiaomo_/article/details/107624429 .exe version

gpu 显存 https://zhuanlan.zhihu.com/p/527143823
https://zhuanlan.zhihu.com/p/493646010
Memory management

PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in nvidia-smi. You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.

For more advanced users, we offer more comprehensive memory benchmarking via memory_stats(). We also offer the capability to capture a complete snapshot of the memory allocator state via memory_snapshot(), which can help you understand the underlying allocation patterns produced by your code.

Use of a caching allocator can interfere with memory checking tools such as cuda-memcheck. To debug memory errors using cuda-memcheck, set PYTORCH_NO_CUDA_MEMORY_CACHING=1 in your environment to disable caching.

The behavior of caching allocator can be controlled via environment variable PYTORCH_CUDA_ALLOC_CONF. The format is PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2><value2>... Available options:

    max_split_size_mb prevents the allocator from splitting blocks larger than this size (in MB). This can help prevent fragmentation and may allow some borderline workloads to complete without running out of memory. Performance cost can range from ‘zero’ to ‘substatial’ depending on allocation patterns. Default value is unlimited, i.e. all blocks can be split. The memory_stats() and memory_summary() methods are useful for tuning. This option should be used as a last resort for a workload that is aborting due to ‘out of memory’ and showing a large amount of inactive split blocks.

    roundup_power2_divisions helps with rounding the requested allocation size to nearest power-2 division and making better use of the blocks. In the current CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512, so this works fine for smaller sizes. However, this can be inefficient for large near-by allocations as each will go to different size of blocks and re-use of those blocks are minimized. This might create lots of unused blocks and will waste GPU memory capacity. This option enables the rounding of allocation size to nearest power-2 division. For example, if we need to round-up size of 1200 and if number of divisions is 4, the size 1200 lies between 1024 and 2048 and if we do 4 divisions between them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200 will be rounded to 1280 as the nearest ceiling of power-2 division.

    garbage_collection_threshold helps actively reclaiming unused GPU memory to avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks), which can be unfavorable to latency-critical GPU applications (e.g., servers). Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80% of the total memory allocated to the GPU application). The algorithm prefers to free old & unused blocks first to avoid freeing blocks that are actively being reused. The threshold value should be between greater than 0.0 and less than 1.0.

重启后修复。win感觉不行

尝试deepin https://mirrors.tuna.tsinghua.edu.cn/deepin-cd/20.6/ 但是感觉不行

# 8.24
决定恢复win10

python 写入格式 https://blog.csdn.net/qq_41224289/article/details/125431747